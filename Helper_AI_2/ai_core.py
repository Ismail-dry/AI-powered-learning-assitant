import os,sys
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, util
import torch
import numpy as np
import re
import traceback

SubjectTitles ='''
BaÅŸlarken
adekoCAM Nedir ?
adekoCAM ile CNC Kesim
Kesim PlanÄ± ve Su YÃ¶nÃ¼
ProsedÃ¼rlerle Ã‡alÄ±ÅŸma
ParÃ§a ve Kapak ArayÃ¼zleri
Kapak TÃ¼retme & Kapak Model Parametreleri
Kapak Modeli Programlama
Kapak Model Parametrelerinin KullanÄ±cÄ±ya AÃ§Ä±lmasÄ±
ÃœrÃ¼n ArayÃ¼zÃ¼
ÃœrÃ¼n Modeli OluÅŸturma
Patron ArayÃ¼zÃ¼
Keyfi ParÃ§a Åekli
Operasyonlar ve TakÄ±m AtamalarÄ±
Kapak ve ModÃ¼l Programlama KomutlarÄ±
YÃ¶netici-OperatÃ¶r Modu EÅŸgÃ¼dÃ¼mÃ¼ - Tracking
Etiket Åablonu DÃ¼zenleme
Malzeme BirleÅŸtirme(iÃ§e aktarÄ±m sÄ±rasÄ±nda)
Ã‡ift YÃ¼zeyinde Ä°ÅŸlem Olan Panelleri Ã‡evirme Ä°ÅŸlemi
KarÅŸÄ±lÄ±klÄ± Deliklerin Boydan Boya Delinme AyarÄ±nÄ±n YapÄ±lÄ±ÅŸÄ±
Makineye ParÃ§a BaÄŸlama
AES Sirius Gibi Makinelere ParÃ§a BaÄŸlama (Z ekseni yere bakan)
adekoCAM GÃ¼ncelleyici'yi Ayarlama
adekoCAM Verilerinin Yedeklenmesi ve YÃ¶netilmesi
Kenar bandÄ± makinesinde aÃ§Ä±lmasÄ± istenilen kanallar iÃ§in kural tanÄ±mlama.
ParÃ§a Ã¶lÃ§Ã¼lerine gÃ¶re parÃ§a listesi csv'si oluÅŸturma
CNC Routerlarda orijin ofsetleme
Makineye Ã¶zel keyfi parametre tanÄ±mlama
adekoCAM'in AÃ§Ä±lmamasÄ± Problemi ve Ã‡Ã¶zÃ¼mleri
'''
# Gemini API anahtarÄ±nÄ±zÄ± ortam deÄŸiÅŸkeni olarak ayarladÄ±ÄŸÄ±nÄ±zdan emin olun.

os.environ["GEMINI_API_KEY"] = "AIzaSyC0wq0jpRhXjBBp9hpQDky9R__ZA4MBvVQ"
class AICore:
    """
    Google Gemini API ile GPT tabanlÄ± Ã¶zetleme ve anlamsal arama iÅŸlemlerini yÃ¶neten Ã§ekirdek sÄ±nÄ±f.
    """

    def __init__(self):
        print("AI Core baÅŸlatÄ±lÄ±yor...")

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"KullanÄ±lacak cihaz: {self.device}")

        # Gemini API
        try:
            # Ortam deÄŸiÅŸkeninden Gemini API anahtarÄ±nÄ± alÄ±yoruz.
            self.api_key = os.getenv("GEMINI_API_KEY")
            if not self.api_key:
                raise EnvironmentError("GEMINI_API_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil.")
            
            # Gemini API'sini yapÄ±landÄ±r
            genai.configure(api_key=self.api_key)
            
           
            self.model = genai.GenerativeModel('gemini-1.5-flash')
            print("Google Gemini modeli baÅŸarÄ±yla yÃ¼klendi.")

        except Exception as e:
            print(f"Hata: Gemini modeli baÅŸlatÄ±lamadÄ±. Detay: {e}")
            self.model = None

       # Anlamsal arama modeli
        try:
            base_path = getattr(sys, '_MEIPASS', os.path.dirname(os.path.abspath(__file__)))
            model_path = os.path.join(base_path, "sbert_model")

            print(f"[DEBUG] Model klasÃ¶rÃ¼ yolu: {model_path}")
            self.search_model = SentenceTransformer(model_path, device=self.device)

            print(f"[DEBUG] Bu klasÃ¶rdeki dosyalar: {os.listdir(model_path)}")


            print("âœ… Anlamsal arama modeli yÃ¼klendi.")
        except Exception as e:
            print("âŒ Anlamsal arama modeli yÃ¼klenemedi.")
            print("[TRACEBACK HATASI]")
            print(traceback.format_exc())
            self.search_model = None

            print("AI Core hazÄ±r.") 

    def summarize_with_gemini(self, text: str, user_prompt: str = None) -> str:
        """
        Google Gemini API ile metni Ã¶zetler.
        """
        if not self.model:
            return "âŒ Gemini modeli yÃ¼klenemedi."

        try:
            # Gemini iÃ§in prompt'u hazÄ±rlÄ±yoruz.
            # Sistem ve kullanÄ±cÄ± rollerini tek bir prompt iÃ§inde birleÅŸtirmek genellikle daha iyi Ã§alÄ±ÅŸÄ±r.
            system_instruction = system_instruction = """Sen Ã¶ÄŸretici uzman bir yardÄ±mcÄ± asistansÄ±n.
KullanÄ±cÄ± sana bir iÅŸlem sorduÄŸunda (Ã¶rneÄŸin nasÄ±l dÃ¼zenlenir, nasÄ±l yapÄ±lÄ±r, nasÄ±l eklenir, nasÄ±l silinir, nasÄ±l deÄŸiÅŸtirilir gibi),
sadece tanÄ±m deÄŸil, adÄ±m adÄ±m iÅŸlem sÄ±rasÄ±nÄ± aÃ§Ä±kla.
Her adÄ±mÄ± sÄ±rasÄ±yla numaralandÄ±r. Gerekiyorsa kod Ã¶rnekleri, buton isimleri, menÃ¼ yollarÄ±, ayarlar veya ilgili kavramlarÄ± da ekle.
CevabÄ±n sade, net, kÄ±sa cÃ¼mleli ve anlaÅŸÄ±lÄ±r olsun. Gereksiz aÃ§Ä±klamalar yapma.
EÄŸer iÃ§erikte adÄ±m adÄ±m bÃ¶lÃ¼mler, 'NasÄ±l yapÄ±lÄ±r', 'ProsedÃ¼r', 'AdÄ±mlar' gibi baÅŸlÄ±klar varsa, Ã¶zellikle onlarÄ± Ã¶n plana Ã§Ä±kar ve kullanÄ±cÄ±nÄ±n amacÄ±na yÃ¶nelik en faydalÄ± cevabÄ± oluÅŸtur.
"""

            
            # EÄŸer Ã¶zel bir kullanÄ±cÄ± prompt'u varsa, onu da ekleyelim.
            if user_prompt:
                if user_prompt:
                    prompt = f"{system_instruction}\n\nKullanÄ±cÄ±nÄ±n sorusu: '{user_prompt}'\n\nLÃ¼tfen aÅŸaÄŸÄ±daki aÃ§Ä±klamayÄ± bu soruya gÃ¶re cevapla ve eÄŸer iÅŸlem adÄ±mlarÄ± varsa sÄ±ralÄ± bir ÅŸekilde detaylandÄ±r:\n\n---\n{text}\n---"
                else:
                    prompt = f"{system_instruction}\n\nVideonun durduÄŸu andaki ifadeyi aÃ§Ä±kla:\n\n---\n{text}\n---"

            else:
                prompt = f"{system_instruction}\n\n Videonun durduÄŸu andaki ifadeyi aÃ§Ä±kla:\n\n---\n{text}\n---"

            # Gemini API'ye istek gÃ¶nder
            response = self.model.generate_content(prompt)
            
            return response.text.strip()
        except Exception as e:
            return f"Gemini Ã¶zetleme hatasÄ±: {e}"

    def find_and_summarize(self, query: str, context_text: str, min_chunk_size: int = 20) -> str:
        """
        Web'den alÄ±nan iÃ§erikte sorguya en uygun kÄ±smÄ± bulur ve Gemini ile Ã¶zetler.
        """
        if not self.search_model:
            return "âŒ Anlamsal arama modeli yÃ¼klenemedi."

        if not context_text or not context_text.strip():
            return "âš ï¸ Analiz edilecek bir metin bulunamadÄ±."

        print(f"'{query}' sorgusu iÃ§in anlamsal arama yapÄ±lÄ±yor...")

        text_chunks = [chunk.strip() for chunk in re.split(r'\n{1,2}', context_text) if len(chunk.strip()) >= min_chunk_size]
        if not text_chunks:
            return f"âš ï¸ Yeterli uzunlukta analiz edilecek bÃ¶lÃ¼m bulunamadÄ±."

        query_embedding = self.search_model.encode(query, convert_to_tensor=True, device=self.device)
        context_embeddings = self.search_model.encode(text_chunks, convert_to_tensor=True, device=self.device)
        search_results = util.semantic_search(query_embedding, context_embeddings, top_k=1)

        if not search_results or not search_results[0]:
            return "ğŸ” Ä°lgili bir bÃ¶lÃ¼m bulunamadÄ±."

        best_match_index = search_results[0][0]["corpus_id"]
        best_match_score = search_results[0][0]["score"]
        relevant_text = text_chunks[best_match_index]

        print(f"âœ”ï¸ En alakalÄ± bÃ¶lÃ¼m bulundu. Benzerlik skoru: {best_match_score:.4f}")
        print(f"[DEBUG] Ã–zetlenecek iÃ§erik:\n{relevant_text[:300]}...\n")

        if "session info" in relevant_text.lower() or "chrome=" in relevant_text.lower():
            return "âš ï¸ GeÃ§ersiz iÃ§erik alÄ±ndÄ±. Sayfa doÄŸru ÅŸekilde yÃ¼klenmemiÅŸ olabilir."
        
        print("ğŸ¤– Gemini ile Ã¶zetleme baÅŸlatÄ±lÄ±yor...")
        # Ã–zetleme fonksiyonunu `summarize_with_gemini` olarak gÃ¼ncelliyoruz.
        # `query`'yi de `user_prompt` olarak gÃ¶ndererek daha odaklÄ± bir Ã¶zet almasÄ±nÄ± saÄŸlÄ±yoruz.
        summary = self.summarize_with_gemini(relevant_text, user_prompt=query)
        return summary
    
    def find_subject_title_with_gemini(self, UserInput: str) -> str:
        """
        Google Gemini API ile metni Ã¶zetler.
        """
        if not self.model:
            return "âŒ Gemini modeli yÃ¼klenemedi."

        try:
            # Gemini iÃ§in prompt'u hazÄ±rlÄ±yoruz.
            # Sistem ve kullanÄ±cÄ± rollerini tek bir prompt iÃ§inde birleÅŸtirmek genellikle daha iyi Ã§alÄ±ÅŸÄ±r.
                system_instruction = f"Sen sadece baÅŸlÄ±k sÃ¶yleme uzmanÄ±sÄ±n. {SubjectTitles}elimizdeki konu baÅŸlÄ±klarÄ± bunlar, bu baÅŸlÄ±klarÄ± Ã¶ÄŸren. Sen sadece buradan {UserInput} gelen cÃ¼mleyle ilgili baÅŸlÄ±klarÄ±n isimlerini sÃ¶yleyeceksin. BaÅŸka bir ÅŸey yazmayacaksÄ±n"
            
            # EÄŸer Ã¶zel bir kullanÄ±cÄ± prompt'u varsa, onu da ekleyelim.
                prompt = f"{system_instruction}"
            
            # Gemini API'ye istek gÃ¶nder
                response = self.model.generate_content(prompt)
                return response.text.strip()
        except Exception as e:
            return f"Gemini Ã¶zetleme hatasÄ±: {e}"